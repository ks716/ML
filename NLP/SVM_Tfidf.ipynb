{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Copy of module_12_assignment_1.ipynb","provenance":[{"file_id":"1WCCVwnls9wJBrWfFXeMNfQVaWSfCQz-T","timestamp":1606317525309}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"wktjz6WAJq3L"},"source":["# Text Classification\n","\n","In this problem, you will be analyzing the Twitter data we extracted using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n","\n","For every tweet, we collected two pieces of information:\n","- `screen_name`: the Twitter handle of the user tweeting and\n","- `text`: the content of the tweet.\n","\n","We divided the tweets into two parts - the train and test sets.  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n","\n","The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n","- **R**: `realDonaldTrump, mike_pence, GOP`\n","- **D**: `HillaryClinton, timkaine, TheDemocrats`\n","\n","We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n","\n","1. **preprocessing**: clean up the raw tweet text using the various functions offered by [the Natural Language Toolkit (`nltk`)](http://www.nltk.org/genindex.html).\n","2. **features**: construct bag-of-words feature vectors.\n","3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n","\n","Note that `nltk` supports optional corpora, toy grammars, trained models, etc. For this assignment, you have to manually install the stopwords list and `WordNetLemmatizer`. We'll begin by installing them:"]},{"cell_type":"markdown","metadata":{"id":"vgm1uWzAODj6"},"source":["Data sets available here:\n","\n","tweets_train : [link](https://drive.google.com/file/d/1blOBpDKkB-h44lbBYdcdLoWw1toieEBQ/view?usp=sharing)\n","\n","tweets_test: [link](https://drive.google.com/file/d/1z_T5bq-I-dYfd3mUlWdWCuZvGVjjMXb2/view?usp=sharing)\n","\n","get_rare_words.output: [link](https://drive.google.com/file/d/11prpQFWCmdKDL27ZVu9THdCOA1TMPVPS/view?usp=sharing)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfd76-QjNzsD","executionInfo":{"status":"ok","timestamp":1606326723552,"user_tz":-330,"elapsed":1743,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"bdc803ba-5dbb-42c6-dd25-c83d2e2e0372"},"source":["!git clone https://github.com/gauravmm/jupyter-testing.git\n","%cd jupyter-testing/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'jupyter-testing'...\n","remote: Enumerating objects: 45, done.\u001b[K\n","remote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects: 100% (23/23), done.\u001b[K\n","remote: Total 45 (delta 17), reused 45 (delta 17), pack-reused 0\n","Unpacking objects: 100% (45/45), done.\n","/content/jupyter-testing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0gpom3gFJq3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606326730517,"user_tz":-330,"elapsed":3713,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"95a78702-d90e-493b-c76c-e707cd182a23"},"source":["import nltk\n","import collections\n","import string\n","import numpy as np\n","import sklearn\n","import gzip\n","import csv\n","import re\n","import matplotlib.pyplot as plt\n","\n","from testing.testing import test\n","\n","def nltk_download_test(nltk_download):\n","    nltk_download()\n","    try:\n","        lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n","        test.true(lemmatizer is not None)\n","        stopwords=nltk.corpus.stopwords.words('english')\n","        test.true(stopwords is not None)\n","    except LookupError:\n","        test.true(False)\n","        \n","@test\n","def nltk_download():\n","    nltk.download('stopwords')\n","    nltk.download('wordnet')\n","    nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","### TESTING nltk_download: PASSED 2/2\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mPcIJAF0Jq3O"},"source":["## 1. Text Processing\n","\n","You first task to fill in the following function which processes and tokenizes raw text. The tokens must:\n","\n","1. be in lower case.\n","2. appear in the same order as in the raw text.\n","3. be in their lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n","4. **not** contain any characters other than numbers and digits; you should:\n","   1. remove trailing `'s`: `Children's` becomes `children`\n","   2. omit other apostrophes: `don't` becomes `dont`\n","   3. break tokens at other punctuation and/or unicode characters: `word-of-mouth` becomes `word`, `of`, `mouth` \n","5. if the lemmatized form is a stopword, it should not appear in the output\n","6. not include the parts of any t.co urls. Many tweets contain URLs from the domain `t.co`; you should strip all such URLs.\n","\n","If you figure out the right order to perform these operations, solving this problem is much easier.\n","\n","**Stopwords** are words that appear very often in text, usually playing a grammatical role (\"and\", \"a\", etc.). When comparing text similarity, these are not very useful; so we eliminate them at this stage. (NLTK provides us with a list of stopwords for English, which we will use later.)\n","\n","Hints:\n","\n"," - you should use `nltk.word_tokenize()` in your solution\n"," - you should break tokens at all characters that are not in `string.ascii_letters` or `string.digits`\n"," - test your URL stripping! It's very easy to make a mistake with it.\n"," - When lemmatizing, you should convert the output to `str` (i.e. `lemmatized_word = str(lemmatizer.lemmatize(word))`) because the lemmatizer may return a non-string object."]},{"cell_type":"code","metadata":{"id":"Y7eemppmrcnb","executionInfo":{"status":"ok","timestamp":1606326741239,"user_tz":-330,"elapsed":935,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}}},"source":["from nltk.stem import WordNetLemmatizer\n","from nltk import word_tokenize\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"weD0vC2bJq3O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606327836159,"user_tz":-330,"elapsed":1018,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"05e98248-e971-4177-a797-6ee654c19fa4"},"source":["def preprocess_test(preprocess):\n","    test.equal(preprocess(\"I'm doing well! How about you?\"), ['im', 'doing', 'well', 'how', 'about', 'you'])\n","    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"),    ['education', 'is', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'losing', 'your', 'temper', 'or', 'your', 'self', 'confidence'])\n","\n","    # Punctuation and space handling\n","    test.equal(preprocess(\" a..a. .a . a.\"), ['a', 'a', 'a', 'a'])\n","    test.equal(preprocess(\"word-of-mouth self-esteem\"), ['word', 'of', 'mouth', 'self', 'esteem'])\n","\n","    # Apostrophe handling\n","    test.equal(preprocess(\"you've\"), ['youve'])\n","    test.equal(preprocess(\"She's\"), ['she'])\n","    test.equal(preprocess(\"SHE'S\"), ['she'])\n","    test.equal(preprocess(\"Cea'sar\"), ['ceaar']) # You can assume that there are no mid-word \"'s\" substrings.\n","\n","    # Lemmatizer\n","    test.equal(preprocess(\"walks\"), ['walk'])\n","    \n","    # Stopwords\n","    stopwords = set(nltk.corpus.stopwords.words('english'))\n","    test.equal(preprocess(\"I'm doing well! How about you?\", stopwords), ['im', 'well'])\n","    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\", stopwords), ['education', 'ability', 'listen', 'almost', 'anything', 'without', 'losing', 'temper', 'self', 'confidence'])\n","\n","    # Unicode handling\n","    test.equal(preprocess(\"dootüëèdoot\"), [\"doot\", \"doot\"])\n","\n","    # URL handling\n","    test.equal(preprocess(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\"), [])\n","    test.equal(preprocess(\"boohttp://t.co/WJs5bmRthUhello\"), [\"boo\", \"hello\"])\n","    test.equal(preprocess(\"SHE'S\"), ['she'])\n","    test.equal(preprocess(\"https://t.co/ZhEy√Ñ¬∂aaaa\"), ['http', 't', 'co', 'zhey', 'aaaa'])\n","    test.equal(preprocess(\"http://t.co/WJs5bmRthU,https://t.co/WJs5bmRthU,\"), [])\n","    test.equal(preprocess(\"https://t.co/WJs5bmRth-\"), ['http', 't', 'co', 'wjs5bmrth'])\n","\n","    # From the training set:\n","    SW=set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"])\n","    test.equal(preprocess('RT @GOPconvention: #Oregon votes today. That means 62 days until the @GOPconvention! https://t.co/OoH9FVb7QS', stopwords=SW), ['gopconvention', 'oregon', 'vote', 'today', 'mean', '62', 'day', 'gopconvention'])\n","    test.equal(preprocess('RT @DWStweets: The choice for 2016 is clear: We need another Democrat in the White House. #DemDebate #WeAreDemocrats http://t.co/0n5g0YN46f', stopwords=SW), ['dwstweets', 'choice', '2016', 'clear', 'need', 'another', 'democrat', 'white', 'house', 'demdebate', 'wearedemocrats'])\n","    test.equal(preprocess(\"Trump's calling for trillion dollar tax cuts for Wall Street.\\n\\nIt's time for them to pay their fair share. https://t.co/y8vyESIOES\", stopwords=SW), ['trump', 'calling', 'trillion', 'dollar', 'tax', 'cut', 'wall', 'street', 'time', 'pay', 'fair', 'share'])\n","    test.equal(preprocess(\".@TimKaine's guiding principle: the belief that you can make a difference through public service. https://t.co/YopSUeMqOX\", stopwords=SW), ['timkaine', 'guiding', 'principle', 'belief', 'make', 'difference', 'public', 'service'])\n","    test.equal(preprocess('Glad the Senate could pass a #THUD / MilCon / VetAffairs approps bill with solid provisions for Virginia: https://t.co/NxIgRC3hDi', stopwords=SW), ['glad', 'senate', 'could', 'pas', 'thud', 'milcon', 'vetaffairs', 'approps', 'bill', 'solid', 'provision', 'virginia'])\n","    test.equal(preprocess('RT @IndyThisWeek: An @rtv6 exclusive: @GovPenceIN sits down with @RafaelOnTV\\nSee it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:‚Ä¶', stopwords=SW), ['indythisweek', 'rtv6', 'exclusive', 'govpencein', 'sits', 'rafaelontv', 'see', 'sunday', 'morning', '8', '30a', 'rtv6', 'rtv6', 'app'])\n","    test.equal(preprocess('From Chatham Town Council to Congress, @RepRobertHurt has made a strong mark on his community. Proud of our work together on behalf of VA!', stopwords=SW), ['chatham', 'town', 'council', 'congress', 'reproberthurt', 'ha', 'made', 'strong', 'mark', 'community', 'proud', 'work', 'together', 'behalf', 'va'])\n","    test.equal(preprocess('Thank you New Orleans, Louisiana!\\n#MakeAmericaGreatAgain #VoteTrump\\nhttps://t.co/tI1h9xT9GX https://t.co/0bf7BOlWEj', stopwords=SW), ['thank', 'new', 'orleans', 'louisiana', 'makeamericagreatagain', 'votetrump'])\n","\n","def remove_special(s):\n","  res=\"\"\n","  for c in s:\n","    if c.isalnum():\n","      res+=c\n","  return res\n","\n","def check_url(url):\n","  if \"http://t.co/\" in url:\n","    sub = url.replace(\"http://t.co/\",\"\")\n","    #print(sub)\n","    sub = re.sub(r'[^\\x00-\\x7F]+',' ', sub) #unicode\n","    new_sub=remove_special(sub)\n","    #print(new_sub)\n","    if len(new_sub)==10:\n","      return True,False\n","    elif len(new_sub)>10:\n","        return True,True\n","    else:\n","      return False,False\n","  if \"https://t.co/\" in url:\n","    sub = url.replace(\"https://t.co/\",\"\")\n","    sub = re.sub(r'[^\\x00-\\x7F]+',' ', sub) #unicode\n","    new_sub=remove_special(sub)\n","    #print(new_sub,len(new_sub))\n","    if len(new_sub)==10:\n","      return True,False\n","    elif len(new_sub)>10:\n","        return True,True\n","    else:\n","      return False,False\n","\n","\n","    \n","\n","\n","@test\n","def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n","    \"\"\" Normalizes case and handles punctuation\n","    \n","    args:\n","        text: str -- raw text\n","        stopwords : Set[str] -- lemmatized tokens to exclude from the output\n","        lemmatizer : Lemmatizer -- an instance of a class implementing the lemmatize() method\n","\n","    Outputs:\n","        list(str): tokenized text\n","    \"\"\"\n","    ### your code goes here\n","    #stand = [\"http://t.co/\",\"https://t.co/\"]\n","    text = text.lower()\n","    #url handling first\n","    text = text.replace(\"http\",\" http\")\n","    #urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n","    urls = re.findall(r\"http[s]*://t.co/[\\w]{10}\", text)\n","    for url in urls:\n","      text = text.replace(url, \" \")\n","    # for i in range(len(urls)):\n","    #   if ',' in urls[i]:\n","    #       text = text.replace(urls[i],\" \")\n","    #   #print(check_url(urls[i]))\n","    #   if check_url(urls[i])!=None:\n","    #     if check_url(urls[i])[0] == True and check_url(urls[i])[1] ==False:\n","    #         text = text.replace(urls[i],\" \")\n","    #     elif check_url(urls[i])[0] == True and check_url(urls[i])[1] ==True:\n","    #         if \"https\" in urls[i]:\n","    #             s = urls[i]\n","    #             s = \" \"+s[23:]\n","    #             text = text.replace(urls[i],s)\n","    #         elif \"http\" in urls[i]:\n","    #             s = urls[i]\n","    #             s = \" \"+s[22:]\n","    #             text = text.replace(urls[i],s)\n","    #     else:\n","    #         #print(urls[i])\n","    #         new = \"\"\n","    #         for c in urls[i]:\n","    #             if c.isalnum():\n","    #                 new+=c\n","    #             else:\n","    #                 new+=\" \"\n","    #         new = re.sub(r'[^\\x00-\\x7F]+',' ', new) #unicode\n","    #         text = text.replace(urls[i],new)\n","    text = re.sub(r'[^\\x00-\\x7F]+',' ', text) #unicode\n","\n","\n","    #text = text.replace(\"https\",\"http\")\n","    # for t in text.split():\n","    #   if \"http\" in t:\n","    #     replace\n","    #text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","    text = text.replace(\"\\n\",\" \")\n","    text = text.replace(\"'s\",\"\")    \n","    text = text.replace(\"'\",\"\")\n","    text = text.replace(\"?\",\"\")\n","    text = text.replace(\"!\",\"\")\n","    text = text.replace(\".\",\" \")\n","    text = text.replace(\"-\",\" \")\n","    text = text.replace(\":\",\" \")\n","    #s = text.split()\n","    tokens = word_tokenize(text)\n","    #lemmatizer = WordNetLemmatizer()\n","    main=[]\n","    for word in tokens:\n","      main.append(lemmatizer.lemmatize(word))\n","    s = [w for w in main if not w in stopwords]\n","    t = [] \n","    for i in range(len(s)):\n","      if s[i].isalnum():\n","        t.append(s[i])\n","    return t\n","    \n","    # ### your code goes here\n","    # #step 0 is remove url\n","    # urlsToRemove = re.findall(r\"http[s]*://t.co/[\\w]{10}\", text)\n","    # for url in urlsToRemove:\n","    #   text = text.replace(url, \" \")\n","    # #step 1 is to set all the words to be all lower case\n","    # text = text.lower()\n","    # #step 2 is to remove all the \"'s\" in the raw text to be null\n","    # text=text.replace(\"'s\",\"\")\n","    # #step 3 is to remove all ' \n","    # text=text.replace(\"'\",\"\")\n","    # #step 4 is remove punctuations\n","    # punctuations = string.punctuation\n","    # for char in punctuations:\n","    #   text = text.replace(char, \" \")\n","    # unicode = string.ascii_letters + string.digits \n","    # #step 6 remove all the invalid characters \n","    # for char in text:\n","    #     if char not in unicode:\n","    #         text = text.replace(char, \" \")\n","    # text = re.sub(r\"[^A-Za-z0-9]+\",\" \",text)\n","    # from nltk.tokenize import word_tokenize\n","    # # step 7 is remove stop words\n","    # tokens = nltk.word_tokenize(text)\n","    # tokens = [lemmatizer.lemmatize(w) for w in tokens]\n","    # tokens = [w for w in tokens if w not in stopwords]\n","    # return tokens"],"execution_count":22,"outputs":[{"output_type":"stream","text":["### TESTING preprocess: PASSED 24/26\n","# 15\t: Failed: ['http', 'aaaa'] is not equal to ['http', 't', 'co', 'zhey', 'aaaa']\n","# 17\t: Failed: ['http'] is not equal to ['http', 't', 'co', 'wjs5bmrth']\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kcXmEcINJq3P"},"source":["We give you some code that uses `preprocess` to prepare the data. This should take no more than 6s to run; if it takes longer than that, you need to make your preprocessing function run quicker."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"21whtjDOzD1N","executionInfo":{"status":"ok","timestamp":1606327530837,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"9a1e5a99-4efe-4980-f7ee-cb34e0e11f02"},"source":["%pwd"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/jupyter-testing'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"tr4lCW2yJq3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606327540653,"user_tz":-330,"elapsed":6341,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"eb487d98-a777-47c6-d77c-d2ae0916971c"},"source":["%timeit\n","## Uncomment the previous line to time your code. Remember to comment it out before uploading your solution.\n","\n","def read_data_test(read_data):\n","    data_train, data_test = read_data()\n","    \n","    test.equal(len(data_train), 17298)\n","    test.equal(len(data_test), 1000)\n","    \n","    #print(data_train[:8])\n","#/content/tweets_test.csv.gz\n","#/content/tweets_train.csv.gz\n","def read_csv(stem, process=lambda x: x):\n","    with gzip.open(f\"/content/{stem}.csv.gz\", \"rt\", newline='', encoding=\"UTF-8\") as file:\n","        csvr = csv.reader(file)\n","        next(csvr)\n","        return list(map(process, csvr))\n","\n","def is_republican(r):\n","    return r in [\"realDonaldTrump\", \"mike_pence\", \"GOP\"]\n","\n","@test\n","def read_data(extra_stopwords=set()):\n","    \"\"\"Reads the dataset from the csv.gz files\n","    \n","    return : Tuple[data_train, data_test]\n","        data_train : List[Tuple[is_republican, tokenized_tweet]]\n","            is_republican : bool -- True if tweet is from a republican\n","            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n","    \"\"\"\n","    stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) | extra_stopwords\n","    data_train = read_csv(\"tweets_train\", process=lambda r: (is_republican(r[0]), preprocess(r[1], stopwords)))\n","    data_test = read_csv(\"tweets_test\", process=lambda r: preprocess(r[0], stopwords))\n","    \n","    return (data_train, data_test)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["### TESTING read_data: PASSED 2/2\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CSkTGy4Jq3P"},"source":["## 2. Feature Construction\n","\n","The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n","\n","The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic:\n","\n","We calculate a frequency distribution of words in the corpus, and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text; we have already removed these. Words with extremely low frequency tend to be typos.\n","\n","We will now implement a function which counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset.\n","\n","(This should take no more than 20s to run, including reading the files.)"]},{"cell_type":"code","metadata":{"id":"wQq9yepMxg-y","executionInfo":{"status":"ok","timestamp":1606327550482,"user_tz":-330,"elapsed":1009,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}}},"source":["from collections import Counter"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFtv4Q5uJq3Q","colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"status":"ok","timestamp":1606327574866,"user_tz":-330,"elapsed":6503,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"c3f654ef-e7f4-4c67-8bb2-7874d130036e"},"source":["def get_distribution_test(get_distribution):\n","    data_train, data_test = read_data()\n","    dist = get_distribution(data_train)\n","    test.true(isinstance(dist, collections.Counter))\n","    if dist is None:\n","        return\n","\n","    # Simple test cases:\n","    test.equal(dist['trump'], 1812)\n","    test.equal(dist['clinton'], 1107)\n","    test.equal(dist['president'], 788)\n","    test.equal(dist['american'], 745)\n","    test.equal(dist['job'], 676)\n","    test.equal(dist['obama'], 438)\n","    test.equal(dist['hoosier'], 393)\n","\n","    # Check that you have the correct number of unique words:\n","    test.equal(len(dist), 16762)\n","    # There are 8048 words used exactly once, 2399 words used twice, etc:\n","    test.equal(collections.Counter(dist.values()).most_common(5), [(1, 8048), (2, 2399), (3, 1198), (4, 778), (5, 577)])\n","    \n","    plt.hist(dist.values(), bins=100)\n","    plt.yscale('log')\n","\n","@test\n","def get_distribution(data_train):\n","    \"\"\" Calculates the word count distribution, excluding stopwords.\n","\n","    args: \n","        data_train -- the training data\n","\n","    return : collections.Counter -- the distribution of word counts\n","    \"\"\"\n","    ### your code goes here\n","    #print(data_train)\n","    SW=set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"])\n","    #new = preprocess(data_train)\n","    words = []\n","    for i in data_train:\n","      words+=i[1]\n","    new = [w for w in words if not w in SW]\n","    #print(words)\n","\n","    return Counter(new)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["### TESTING get_distribution: PASSED 2/10\n","# 1\t: Failed: 1800 is not equal to 1812\n","# 2\t: Failed: 1106 is not equal to 1107\n","# 4\t: Failed: 744 is not equal to 745\n","# 5\t: Failed: 673 is not equal to 676\n","# 6\t: Failed: 436 is not equal to 438\n","# 7\t: Failed: 392 is not equal to 393\n","# 8\t: Failed: 16465 is not equal to 16762\n","# 9\t: Failed: [(1, 7877), (2, 2348), (3, 1187), (4, 770), (5, 583)] is not equal to [(1, 8048), (2, 2399), (3, 1198), (4, 778), (5, 577)]\n","###\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"IL27UXOhJq3Q"},"source":["Notice the distribution looks exponential, even with a logarithmic y-axis; there are a lot words that appear only once. Lets figure out what these words are so we can eliminate them from the dataset."]},{"cell_type":"code","metadata":{"id":"ex_0u2YRJq3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606316838522,"user_tz":-330,"elapsed":5692,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"bf0c0713-621a-4ba3-defd-8ac9d883c45f"},"source":["def get_rare_words_test(get_rare_words):\n","    data_train, data_test = read_data()\n","    dist = get_distribution(data_train)\n","    new_stopwords = get_rare_words(dist)\n","    \n","    with open(\"/content/get_rare_words.output\", \"rt\", newline=\"\") as f:\n","        ref_stopwords = set(t.strip() for t in f)\n","\n","    test.true(isinstance(new_stopwords, set))\n","    # Extra words in your solution:\n","    test.equal(new_stopwords - ref_stopwords, set())\n","    # Words missing from your solution:\n","    test.equal(ref_stopwords - new_stopwords, set())\n","\n","@test\n","def get_rare_words(dist):\n","    \"\"\"use the word count information from the training data to find more stopwords\n","\n","    args:\n","        dist: collections.Counter -- the output of get_distribution\n","\n","    returns : Set[str] -- a set of all words that appear exactly once in the training data\n","    \"\"\"\n","    ### your code goes here\n","    rare_word = []\n","\n","    for i in list(dist.keys()):\n","      if dist[i]==1:\n","        rare_word.append(i)\n","    \n","    return set(rare_word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["### TESTING get_rare_words: PASSED 3/3\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KqTGXIzJJq3R"},"source":["Here we provide a wrapper function to cache the preprocessed data. This helps it not take quite as long to re-run. If you change anything above this cell, re-run this cell to clear the cache."]},{"cell_type":"code","metadata":{"id":"M_OA9BhVJq3R"},"source":["global PREPROCESSED_DATA_CACHE\n","PREPROCESSED_DATA_CACHE = None\n","\n","def get_data():\n","    global PREPROCESSED_DATA_CACHE\n","    if PREPROCESSED_DATA_CACHE is None:\n","        data_train, data_test = read_data()\n","        dist = get_distribution(data_train)\n","        new_stopwords = get_rare_words(dist)\n","        PREPROCESSED_DATA_CACHE = read_data(new_stopwords)\n","\n","    return PREPROCESSED_DATA_CACHE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PrLNQK-Jq3R"},"source":["### Vectorizing\n","\n","Now we have each tweet as a list of words, excluding words with high- and low-frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n","\n","There's a catch, though: `TfidfVectorizer` expects the input to be a string, and (by default) it perfoms its own analyzing. You have to override that behavior by passing in `do_nothing` to the constructor as an optional parameter.\n","\n","Hints:\n","\n"," - Read [the documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) carefully, and then this [blog post](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/) ([mirror](http://archive.is/pVdqE)). You need to pass in `do_nothing` in two locations.\n"," - You should use only the training data to `fit` or `fit_transform` the vectorizer."]},{"cell_type":"code","metadata":{"id":"20Ds4M0U6SOV"},"source":["import scipy.sparse as sp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0ZdfzVBJq3S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606317352596,"user_tz":-330,"elapsed":1374,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"0b7557b6-0e27-4de7-bc3b-c7b4a3092f24"},"source":["# Helper function, do not change:\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def do_nothing(x):\n","    return x\n","\n","def create_features_test(create_features):\n","    train_features, train_labels, test_features = create_features(*get_data())\n","\n","    test.equal(repr(train_features), \"\"\"<17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 161480 stored elements in Compressed Sparse Row format>\"\"\")\n","\n","    test.equal(repr(test_features), \"\"\"<1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 9037 stored elements in Compressed Sparse Row format>\"\"\")\n","\n","    test.equal(train_labels.dtype, bool)\n","    test.equal(len(train_labels), 17298)\n","    test.equal(sum(train_labels), 8646)\n","\n","@test\n","def create_features(train_data, test_data):\n","    \"\"\"creates the feature matrices and label vector for the training and test sets.\n","\n","    args:\n","        train_data : List[Tuple[is_republican, tweet_words]]\n","            is_republican : bool -- True if Republican, False otherwise\n","            tweet_words : List[str] -- the processed tweet tokens\n","        test_data : List[List[str]] -- a list of processed tweets\n","\n","    returns: Tuple[train_features, train_labels, test_features]\n","        train_features : scipy.sparse.csr.csr_matrix -- feature matrix for the training set\n","        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n","        test_features : scipy.sparse.csr.csr_matrix -- feature matrix for the test set\n","    \"\"\"\n","\n","    train_labels = None\n","    train_features = None\n","    test_features = None\n","\n","    ### Your code goes here\n","    tfidf = TfidfVectorizer(analyzer ='word',tokenizer=do_nothing,preprocessor = do_nothing, token_pattern=None)\n","    words = []\n","    label=[]\n","    print(len(train_data))\n","    for i in train_data:\n","      words.append(i[1])\n","      #print(i[0])\n","      if i[0]==False:\n","        label.append(0)\n","      else:\n","        label.append(1)\n","      #label.append(str(i[0]))\n","    train_features = tfidf.fit_transform(words)\n","    #treain_features = train_features.vocabulary_()\n","    #train_features = sp.csr_matrix(train_features)\n","    train_labels = np.array(label,dtype=bool)\n","    #tfidf = TfidfVectorizer(analyzer ='word',tokenizer=do_nothing,preprocessor = do_nothing, token_pattern=None)\n","    test_features = tfidf.transform(test_data)\n","\n","\n","    return (train_features, train_labels, test_features)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["17298\n","### TESTING create_features: PASSED 5/5\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TiROS1AfJq3S"},"source":["Observe that the created matrices are very sparse.\n","\n","Now that we have the features, lets perform the classification:"]},{"cell_type":"markdown","metadata":{"id":"M6lbaE_RJq3S"},"source":["## 3. Classification\n","\n","We are ready to put it all together and train the classification model.\n","\n","You will be will be using the Support Vector Machine [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). [Here](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html) is a quick introduction to SVMs.\n","\n","At the heart of an SVM is the concept of a _kernel function_, which determines the distance between two data points. `sklearn.svm.SVC` natively supports four kernel functions: `linear`, `poly`, `rbf`, `sigmoid`. For this problem space, we will use the `linear` kernel.\n","\n","In this section, we will:\n","\n","1. build a classifier using the `linear` kernel,\n","2. train it using the training set,\n","3. evaluate the trained model on the training set, and then\n","4. use it to predict classification on our test set.\n","\n","Let's begin by training a classifier. This should take no more than 20s to run. You should set the optional parameter `gamma` to `auto`, but leave the rest at their default values."]},{"cell_type":"code","metadata":{"id":"ul1luLWlJq3S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606317413552,"user_tz":-330,"elapsed":22312,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"1ad85038-5926-4dd1-d254-13010c247a75"},"source":["from sklearn.svm import SVC\n","\n","def learn_classifier_test(learn_classifier):\n","    train_features, train_labels, _ = create_features(*get_data())\n","    classifier = learn_classifier(train_features, train_labels)\n","    test.equal(repr(classifier).replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"  \", \" \"), \"\"\"SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\"\"\")\n","\n","@test\n","def learn_classifier(train_features, train_labels, kernel=\"linear\"):\n","    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n","\n","    args:\n","        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n","        train_labels : numpy.ndarray(bool): binary vector of class labels\n","        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n","\n","    return : sklearn.svm.classes.SVC -- classifier\n","    \"\"\"\n","    ## Uncomment the below code to use the SVM algorithm, run to analyse and pass the testcases\n","    assert kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n","    \n","    clf = SVC(kernel = kernel, gamma = 'auto')\n","    clf.fit(train_features, train_labels) \n","    \n","    return clf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17298\n","### TESTING learn_classifier: PASSED 1/1\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f_HkCk4OJq3S"},"source":["Now that we know how to train a classifier, the next step is to measure its performance. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n","\n","We would ordinarily use a held-out validation set to evaluate the performance of the classifier. The use of a held-out set prevents overfitting to the data, and you will do this for another assignment. For this problem, though, we can use the training set.\n","\n","To measure classification accuracy we will use the [$F_1$ score](https://en.wikipedia.org/wiki/F1_score). Implement this:"]},{"cell_type":"code","metadata":{"id":"ITu-uweuJq3S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606317444034,"user_tz":-330,"elapsed":1396,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"cf0d35a9-de94-4a35-b289-ff97e62518c4"},"source":["def f1_test(f1):\n","    test.equal(f1([1,1,1], [1,1,1]), 1.0)\n","    test.equal(f1([1,0,0], [1,0,0]), 1.0)\n","    test.equal(f1([1,1,0], [1,1,1]), 0.8)\n","    test.equal(f1([1,0,0], [1,1,0]), 2/3)\n","    test.equal(f1([0,0,1], [1,0,1]), 2/3)\n","    test.equal(f1([1,0,0], [1,1,1]), 0.5)\n","\n","@test\n","def f1(pred, ground):\n","    \"\"\" evaluates a classifier based on a supplied validation data\n","\n","    args:\n","        pred: numpy.ndarray(bool) -- predictions\n","        ground: numpy.ndarray(bool) -- known ground-truth values\n","    \n","    return : double -- the F1 score of the predictions\n","    \"\"\"\n","    ## Uncomment the below code, run to analyse and pass the testcases\n","    pred = np.array(pred, dtype=bool)\n","    ground = np.array(ground, dtype=bool)\n","    tp = np.sum((pred == 1) & (ground == 1).ravel()).astype(float)\n","    fp = np.sum((pred == 1) & (ground == 0).ravel()).astype(float)\n","    fn = np.sum((pred == 0) & (ground == 1).ravel()).astype(float)\n","    precision = tp/(tp+fp)\n","    recision = tp/(tp+fn)\n","    f1 = (2 * precision * recision)/(precision + recision)\n","    \n","    return f1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["### TESTING f1: PASSED 6/6\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pQLeYMhQJq3S"},"source":["Now we calculate the F1 score on the training set:"]},{"cell_type":"code","metadata":{"id":"lprzwXMoJq3S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606317482153,"user_tz":-330,"elapsed":34587,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"cb122324-c55a-41f8-d4a3-42cf2f38a231"},"source":["def evaluate_test(evaluate):\n","    train_features, train_labels, _ = create_features(*get_data())\n","    test.true(np.abs(evaluate(train_features, train_labels, 'linear') - 0.95389842) < 1e-5)\n","\n","@test\n","def evaluate(train_features, train_labels, kernel=\"linear\"):\n","    \"\"\"train the classifier and report the F1 score on the training set\n","    \n","    args:\n","        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n","        train_labels : numpy.ndarray(bool): binary vector of class labels\n","        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n","\n","    return : double -- the F1 score of the predictions on the training labels\n","    \"\"\"\n","    ## Uncomment the below code, run to analyse and pass the testcases\n","    classifier = learn_classifier(train_features, train_labels)\n","    predict = classifier.predict(train_features)\n","    return f1(predict, train_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17298\n","### TESTING evaluate: PASSED 1/1\n","###\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H3dcBnHzJq3T"},"source":["### Classifying Test Tweets\n","\n","Home stretch! Now we can classify the test tweets! Use `learn_classifier` to make a trained classifier and predict the labels given the `test_features`."]},{"cell_type":"code","metadata":{"id":"7knA13pKJq3T","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1606317509173,"user_tz":-330,"elapsed":22012,"user":{"displayName":"Sreenivas Kanaparthy","photoUrl":"","userId":"09508478944846816165"}},"outputId":"4b5324cb-7113-4d4a-ad5d-4a232e4ba28e"},"source":["def pp(entries):\n","    from IPython.display import HTML, display\n","    import tabulate\n","\n","    display(HTML(tabulate.tabulate([(f'<b>{\"R\" if isr else \"D\"}</b>', txt[0]) for isr, txt in entries], tablefmt='html')))\n","\n","def classify_tweets_test(classify_tweets):\n","    test_original = read_csv(\"tweets_test\")\n","    train_features, train_labels, test_features = create_features(*get_data())\n","    test_classes = classify_tweets(train_features, train_labels, test_features)\n","\n","    pp([e for i, e in enumerate(zip(test_classes, test_original)) if i in [0, 2, 9, 70, 654, 723]])\n","\n","@test\n","def classify_tweets(train_features, train_labels, test_features):\n","    \"\"\"Train a model and predict class labels for the test set.\n","\n","    args:\n","        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n","        train_labels : numpy.ndarray(bool): binary vector of class labels\n","        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features, test set\n","\n","    return : numpy.ndarray[bool] -- True if the corresponding tweet is predicted to be Republican, False otherwise.\n","    \"\"\"\n","    ## Uncomment the below code, run to analyse and pass the testcases\n","    classifier = learn_classifier(train_features, train_labels)\n","    predict = classifier.predict(test_features)\n","    return predict"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17298\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table>\n","<tbody>\n","<tr><td>&lt;b&gt;D&lt;/b&gt;</td><td>A comprehensive look at the many lies and offenses of Donald Trump: https://t.co/HKY6HxxFUX https://t.co/cF5GsywU3f                            </td></tr>\n","<tr><td>&lt;b&gt;D&lt;/b&gt;</td><td>&quot;I‚Äôm here as a proud American, a proud Democrat, a proud mother, and tonight, in particular, a very, very proud daughter.‚Äù ‚Äî@ChelseaClinton    </td></tr>\n","<tr><td>&lt;b&gt;R&lt;/b&gt;</td><td>Oops! Clinton confuses the Constitution with the Declaration of Independence &amp;amp; backs a constitutional right to life.\n","https://t.co/gG6xbptUyo                                                                                                                                                </td></tr>\n","<tr><td>&lt;b&gt;R&lt;/b&gt;</td><td>Secret Server you need to wipe clean? http://t.co/oHlxKqImWB Get Hillary&#x27;s Secret Server Wiper today. http://t.co/ANbo9R6Qwt                   </td></tr>\n","<tr><td>&lt;b&gt;D&lt;/b&gt;</td><td>&quot;My dad ran a union ironworking shop...my mom was his best salesman. My brothers &amp;amp; I pitched in...that&#x27;s how small family businesses do it&quot;</td></tr>\n","<tr><td>&lt;b&gt;D&lt;/b&gt;</td><td>Thomas Jefferson loved vanilla ice cream. He brought home a recipe from France, which is now in the @librarycongress #VAisForPresidents        </td></tr>\n","</tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["### TESTING classify_tweets: PASSED 0/0\n","###\n","\n"],"name":"stdout"}]}]}